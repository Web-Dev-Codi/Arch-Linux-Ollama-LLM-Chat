[app]
title = "OllamaTerm"
class = "ollamaterm"
connection_check_interval_seconds = 15

[ollama]
host = "http://localhost:11434"
model = "llama3.2"
models = ["llama3.2", "qwen2.5", "mistral"]
timeout = 120
system_prompt = "You are a helpful assistant."
max_history_messages = 200
max_context_tokens = 4096
pull_model_on_start = true

[ui]
font_size = 14
background_color = "#1a1b26"
user_message_color = "#7aa2f7"
assistant_message_color = "#9ece6a"
border_color = "#565f89"
show_timestamps = true
stream_chunk_size = 8

[keybinds]
send_message = "ctrl+enter"
new_conversation = "ctrl+n"
quit = "ctrl+q"
scroll_up = "ctrl+k"
scroll_down = "ctrl+j"
command_palette = "ctrl+p"
toggle_model_picker = "ctrl+m"
save_conversation = "ctrl+s"
load_conversation = "ctrl+l"
export_conversation = "ctrl+e"
search_messages = "ctrl+f"
copy_last_message = "ctrl+y"

[security]
allow_remote_hosts = false
allowed_hosts = ["localhost", "127.0.0.1", "::1"]

[logging]
level = "INFO"
structured = true
log_to_file = false
log_file_path = "~/.local/state/ollamaterm/app.log"

[persistence]
enabled = false
directory = "~/.local/state/ollamaterm/conversations"
metadata_path = "~/.local/state/ollamaterm/conversations/index.json"

[capabilities]
# Whether to render the model's reasoning trace in the assistant bubble.
# The trace is shown only when the active model supports thinking (auto-detected).
show_thinking = true

# Enable Ollama's built-in web_search and web_fetch tools.
# Requires OLLAMA_API_KEY to be set (or web_search_api_key below).
# Only active when the model also supports tool calling (auto-detected).
# See: https://ollama.com/settings/keys
web_search_enabled = false

# API key for Ollama web search. If empty, falls back to OLLAMA_API_KEY env var.
web_search_api_key = ""

# Maximum number of tool-call iterations per message before stopping the loop.
max_tool_iterations = 10

# NOTE: thinking support, tool calling, and vision are now detected automatically
# from Ollama's /api/show response and no longer require manual configuration.
